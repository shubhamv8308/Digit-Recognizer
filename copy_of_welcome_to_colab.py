# -*- coding: utf-8 -*-
"""Copy of Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kqe1LzU6BISts-iZcPMdfEdzCs1N6VNo
"""

import numpy as np
import pandas as pd
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

raw_train_data = pd.read_csv('/content/train.csv')
raw_train_data.head()

img_rows, img_cols = 28, 28
num_classes = 10

def data_prep(df):
    label_1h = to_categorical(df.label, num_classes)
    num_images = df.shape[0]
    x_as_array = df.iloc[:, 1:].to_numpy()
    x_as_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)
    return x_as_array, label_1h

X, y = data_prep(raw_train_data)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, train_size = 0.8, stratify = y)

def plot_img(array, labels):
    """Return an image with one_hot encoded labels."""
    plt.figure(figsize = (9, 9))
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        plt.title(str(labels[i]))
        plt.imshow(array[i], cmap = 'gray')
        plt.axis('off')

plot_img(X_train, y_train)

!pip install keras-tuner

import keras_tuner
from keras import Sequential, layers
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.layers import Input, Conv2D, Dense, Rescaling, Flatten, MaxPooling2D, Dropout
from keras.optimizers import Adam

def model_builder(hp):
    model = Sequential()
    model.add(Input(shape = (28, 28, 1)))
    model.add(Rescaling(scale = 1./255))
    model.add(layers.RandomFlip("horizontal"))
    model.add(layers.RandomRotation(0.1))

    for i in range(hp.Int("layer", min_value=3, max_value=7)):
      model.add(Conv2D(filters=hp.Choice(f"filters_{i}", [32, 64, 128, 256, 512]),kernel_size=hp.Int("kernel_size", min_value=2, max_value=5),padding=hp.Choice("padding", ["same", "valid"]), activation=hp.Choice("activation", ["relu", "tahn", "sigmoid"])))

      if hp.Boolean(f"max_pool_{i}"):
          model.add(MaxPooling2D())

    if hp.Boolean("Dropout"):
        model.add(Dropout(0.3))
    model.add(Flatten())
    model.add(Dense(10, activation = 'softmax'))
    model.compile(optimizer = Adam(learning_rate=0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])
    return model

def fixed_params(hp):
    hp.Fixed("layer", value=4)
    hp.Fixed("filters_0", value=32)
    hp.Fixed("filters_1", value=128)
    hp.Fixed("filters_2", value=256)
    hp.Fixed("filters_3", value=512)
    hp.Fixed("kernel_size", value=3)
    hp.Fixed("padding", value="same")
    hp.Fixed("activation", value="relu")
    hp.Fixed("max_pool_0", value=True)
    hp.Fixed("max_pool_1", value=True)
    hp.Fixed("max_pool_2", value=True)
    hp.Fixed("max_pool_3", value=False)
    hp.Fixed("Dropout", value=True)
    return hp


tuner = keras_tuner.GridSearch(hypermodel = model_builder,
                                objective = 'val_loss',
                                hyperparameters=fixed_params(keras_tuner.HyperParameters()),
                                tune_new_entries=True,
                                max_trials = None,
                                overwrite=True,
                                directory="/kaggle/working/",
                                project_name="test")
tuner.search_space_summary()

tuner.search(X_train, y_train,
             epochs=100, validation_split=0.3,
             batch_size = 32,
             callbacks = [EarlyStopping(monitor="val_loss",patience=15),
                         ReduceLROnPlateau(monitor="val_loss",patience=7)]
            )

tuner.results_summary(5)

history = tuner.get_best_models()[0]

history.evaluate(X_test, y_test)

raw_test_data = pd.read_csv("/content/train.csv")
raw_test_data.head()

num_img = raw_test_data.shape[0]
test_data = raw_test_data.to_numpy()
test_X = test_data[:, 1:].reshape(num_img, img_rows, img_cols)

preds = history.predict(test_X)

submission_df = pd.DataFrame({"ImageId":range(1, preds.shape[0]+1),
                            "Label":np.argmax(preds, axis = 1)})
submission_df.head()

submission_df.to_csv("submission.csv",index=False)

_, accuracy = history.evaluate(X_test, y_test)
print('Accuracy: %.2f' % (accuracy*100))

import matplotlib.pyplot as plt
import numpy as np
# Assuming 'history' is your trained model
predictions = history.predict(X_test)

# Get predicted labels
predicted_labels = np.argmax(predictions, axis=1)

# Plot some test images with their predicted labels
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_test[i].reshape(28,28), cmap=plt.cm.binary)
    plt.xlabel("Predicted: {}".format(predicted_labels[i]))
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Get true labels from the test set
true_labels = np.argmax(y_test, axis=1)

# Calculate the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report

# Get true labels from the test set
true_labels = np.argmax(y_test, axis=1)

# Calculate the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Print classification report
print(classification_report(true_labels, predicted_labels))

